{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spark session 22 February _Puranjay Kwatra"
      ],
      "metadata": {
        "id": "zJ3KtR99w7vs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "G-wnyuSquH29",
        "outputId": "c2bf8efa-5150-4013-d22e-1280a9c12f17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7902ec766290>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://05612cfaf789:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a spark session named spark"
      ],
      "metadata": {
        "id": "Y12NKH94xNOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "hEVLEhkYuLQI",
        "outputId": "d50fa6cc-dd0b-4b98-8a83-83ebb363c8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7902ec766290>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://05612cfaf789:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading the data from the text file (country code Data)"
      ],
      "metadata": {
        "id": "5-MI7DeMuVdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.options(delimiter='|',infeschema=True,header=True).csv(\"/content/file.csv.txt\")\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDdxQ-iVuLzI",
        "outputId": "82d9a123-faeb-47d3-d65f-6d9431b6d913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+----+-------+\n",
            "|Country Name|Country Code|Year|Value  |\n",
            "+------------+------------+----+-------+\n",
            "|       India|        In01|2000|     90|\n",
            "|         USA|        US03|2001|     18|\n",
            "|       China|        ch07|1999|     78|\n",
            "|       Japan|       jap82|2005|     45|\n",
            "|       Saudi|       sau81|2003|     56|\n",
            "+------------+------------+----+-------+\n",
            "\n",
            "root\n",
            " |-- Country Name: string (nullable = true)\n",
            " |-- Country Code: string (nullable = true)\n",
            " |-- Year: string (nullable = true)\n",
            " |-- Value  : string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sorting and order"
      ],
      "metadata": {
        "id": "YF9OrIgZvzLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"Country Code\",\"Value  \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkiJ6l4rvw3d",
        "outputId": "ffdb2078-f678-4980-84ba-262121c447e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------+\n",
            "|Country Code|Value  |\n",
            "+------------+-------+\n",
            "|        In01|     90|\n",
            "|        US03|     18|\n",
            "|        ch07|     78|\n",
            "|       jap82|     45|\n",
            "|       sau81|     56|\n",
            "+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using orderBy"
      ],
      "metadata": {
        "id": "Y0AkQ2OtwEWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('Country','Value  ').orderBy('Value  ',ascending=False).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-_c4OCLwDXp",
        "outputId": "58c67bcf-06e8-4bdd-91f5-bdbeab914af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+\n",
            "|Country|Value  |\n",
            "+-------+-------+\n",
            "|  India|     90|\n",
            "|  China|     78|\n",
            "|  Saudi|     56|\n",
            "|  Japan|     45|\n",
            "|    USA|     18|\n",
            "+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Condition for year in 2000 OR 2005"
      ],
      "metadata": {
        "id": "f3OiGgwou3AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "filtered_dataframe =df.filter(col('Year').isin(2000,2005) )\n",
        "filtered_dataframe.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4brK5BqKu5l2",
        "outputId": "dae413b2-947a-4cb5-94b0-8119440451b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+----+-------+\n",
            "|Country Name|Country Code|Year|Value  |\n",
            "+------------+------------+----+-------+\n",
            "|       India|        In01|2000|     90|\n",
            "|       Japan|       jap82|2005|     45|\n",
            "+------------+------------+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Condition for year in 2000 and 2005"
      ],
      "metadata": {
        "id": "9xAsWJDYvDJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "filtered_dataframe = df.filter((col('Year') == 2000) & (col('Year') == 2005))\n",
        "filtered_dataframe.show()\n",
        "# Empty result because there is no matching column"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJyeRv0Zu8Og",
        "outputId": "c52bce83-ffeb-4bdf-f404-f72b8b2b6da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+----+-------+\n",
            "|Country Name|Country Code|Year|Value  |\n",
            "+------------+------------+----+-------+\n",
            "+------------+------------+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter in Pyspark"
      ],
      "metadata": {
        "id": "xcp0aEDevKg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "filtered_dataframe =df.filter(col('Country Name')=='India' )\n",
        "filtered_dataframe.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUrk03HLvNHy",
        "outputId": "ea0d9b7a-4280-4e3c-a724-a671df662306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+----+-------+\n",
            "|Country Name|Country Code|Year|Value  |\n",
            "+------------+------------+----+-------+\n",
            "|       India|        In01|2000|     90|\n",
            "+------------+------------+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column renamed function\n"
      ],
      "metadata": {
        "id": "LGh7rZXJvkm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new_data={'country':'countryname'}\n",
        "df =df.withColumnRenamed(\"Country Name\",\"Country\")\n",
        "# df_renamed.show()\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBA-nZbmvf_e",
        "outputId": "dbddf854-7660-4c78-a9d2-443f1bfb43b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+----+-------+\n",
            "|Country|Country Code|Year|Value  |\n",
            "+-------+------------+----+-------+\n",
            "|  India|        In01|2000|     90|\n",
            "|    USA|        US03|2001|     18|\n",
            "|  China|        ch07|1999|     78|\n",
            "|  Japan|       jap82|2005|     45|\n",
            "|  Saudi|       sau81|2003|     56|\n",
            "+-------+------------+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter,groupby and aggregation all in One line"
      ],
      "metadata": {
        "id": "YIqdhUtqvO_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_frame = df.filter(col('Year') == 2000).groupby(\"Value  \").agg({'Value  ':'count'})\n",
        "new_data_frame.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCS8pbF4vVNT",
        "outputId": "76af26e4-2cb6-4b98-950c-c00c1c1550dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+\n",
            "|Value  |count(Value  )|\n",
            "+-------+--------------+\n",
            "|     90|             1|\n",
            "+-------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changing names of Multiple Columns"
      ],
      "metadata": {
        "id": "8BuDG4-YwWfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rename = {\"Country\" : \"Country Name\", \"Country Code\" : \"Code\"}\n",
        "df.toDF(*[rename.get(col, col) for col in df.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPKjvnEovbaG",
        "outputId": "574ad4c1-1623-4cda-b29f-7015262d7a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----+----+-------+\n",
            "|Country Name| Code|Year|Value  |\n",
            "+------------+-----+----+-------+\n",
            "|       India| In01|2000|     90|\n",
            "|         USA| US03|2001|     18|\n",
            "|       China| ch07|1999|     78|\n",
            "|       Japan|jap82|2005|     45|\n",
            "|       Saudi|sau81|2003|     56|\n",
            "+------------+-----+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partioning,re-partioining and Shuffling\n",
        "\n",
        "Shuffling- The Spark SQL shuffle is a mechanism for redistributing or re-partitioning data so that the data is grouped differently across partitions. Based on your data size you may need to reduce or increase the number of partitions of RDD/DataFrame using spark.sql.shuffle.partitions configuration or through code.\n"
      ],
      "metadata": {
        "id": "2buf0gH_wnrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import required modules\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"hash\\_partitioning\").getOrCreate()\n",
        "\n",
        "# Create a sample DataFrame\n",
        "df = spark.createDataFrame([\n",
        "    (1, \"Alice\", 25),\n",
        "    (2, \"Bob\", 30),\n",
        "    (3, \"Charlie\", 35),\n",
        "    (4, \"Dave\", 40),\n",
        "    (5, \"Eve\", 45),\n",
        "    (6, \"Frank\", 50)\n",
        "], [\"id\", \"name\", \"age\"])\n",
        "\n",
        "# Print the DataFrame\n",
        "df.show()\n",
        "# Perform hash partitioning on the\n",
        "# DataFrame based on the \"id\" column\n",
        "df = df.repartition(4, \"id\")\n",
        "\n",
        "# Print the elements in each partition\n",
        "print(df.rdd.glom().collect())"
      ],
      "metadata": {
        "id": "MNURye40wf7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49bd552-7133-43dc-cc51-5c892edadbd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|  Alice| 25|\n",
            "|  2|    Bob| 30|\n",
            "|  3|Charlie| 35|\n",
            "|  4|   Dave| 40|\n",
            "|  5|    Eve| 45|\n",
            "|  6|  Frank| 50|\n",
            "+---+-------+---+\n",
            "\n",
            "[[Row(id=2, name='Bob', age=30), Row(id=4, name='Dave', age=40), Row(id=5, name='Eve', age=45)], [Row(id=1, name='Alice', age=25), Row(id=6, name='Frank', age=50)], [], [Row(id=3, name='Charlie', age=35)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m9YXCSHuRnqj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}