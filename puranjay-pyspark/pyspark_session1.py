# -*- coding: utf-8 -*-
"""Pyspark_Session1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JJXxbdirov4K3yAyUeWPQTUXSFjoz6Ke
"""

# !apt-get install openjdk-8-jdk-headless -qq > /dev/null
# !wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
# !tar xf spark-3.1.1-bin-hadoop3.2.tgz
# !pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"

import findspark
findspark.init()

from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark

"""initail thing in the spark application is creating a sparkContext object which tells the applocation hwo to access a cluster."""

from pyspark.conf import SparkConf
from pyspark.context import SparkContext
conf =SparkConf().setAppName('Pyspark Demo app').setMaster('local[2]')
conf.get('spark.master')
conf.get('spark.app.name')

"""what is **spark context** ?
it is an entry gate for any spark derived application or functionality. It is avaialble as sc by desfault in pyspark
"""

import findspark
findspark.init()
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.sql('''select 'spark' as hello''')
df.show()

# Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql import Row

# Create a SparkSession
spark = SparkSession.builder \
    .appName("Simple PySpark DataFrame") \
    .getOrCreate()

# Create some data
data = [
    Row(id=1, name='Puranjay', age=30),
    Row(id=2, name='abhi', age=25),
    Row(id=3, name='Riya', age=35)
]

# Create a DataFrame from the data
df = spark.createDataFrame(data)

# Show the DataFrame
df.show()

from pyspark.sql.functions import col
df = df.withColumn("age", col("age").cast("string"))

"""# Read the CSV with inferschema
### InferSchema will guess the dataType of the source file and show us the datatype
"""

from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .appName("Read CSV") \
    .getOrCreate()
df = spark.read.csv("/content/people-100.csv", header=True, inferSchema=False)

df.show(20)

df.printSchema()

# from pyspark.sql import SparkSession
# from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# # Create a SparkSession
# spark = SparkSession.builder \
#     .appName("CreateDataFrameExample") \
#     .getOrCreate()

# # Define schema for the DataFrame
# schema = StructType([
#     StructField("id", IntegerType(), True),
#     StructField("name", StringType(), True),
#     StructField("age", IntegerType(), True)
# ])

# # Sample data
# data = [(1, "Harsh", 30),
#         (2, "Yash", 25),
#         (3, "Janit", 35)]

# # Create a DataFrame
# df = spark.createDataFrame(data, schema)

# # Show the DataFrame
# df.show()

df.printSchema()

from pyspark.sql.functions import col
df = df.withColumn("age", col("age").cast("string"))

df.printSchema()

sc=spark.sparkContext
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

print(rdd.collect())

df2 = spark.read.parquet("/content/userdata1.parquet")

df2.show(5)

df2.printSchema()

#read the json file by taking the multiline=true
df3=spark.read.json("/content/example_1.json")

!cat /content/example_1.json

df3.printSchema()

df3 = df3.filter(df3["_corrupt_record"].isNull())

#read the json file by taking the multiline=true

df4=spark.read.json("/content/example_1.json")

!cat /content/example_1.json

"""As you can see, when multiline=false, PySpark reads the entire file as a single JSON object and stores it in a column named _corrupt_record, because the data is not formatted correctly for this setting. This is why it's important to choose the appropriate value for the multiline option based on the structure of your JSON data."""

df5 = spark.read.option("multiline", "false").json("/content/sample_data/anscombe.json")
df5.show()

"""#
Reading the JSON file

"""

df6 = spark.read.option("multiline", "true").json("/content/example_2.json")
df6.show()

df5.printSchema()

df5.columns

"""Truncate=When truncate is set to True (which is the default behavior if not specified), the displayed strings in the columns are truncated if they exceed a certain length, and ... is appended at the end of the truncated strings."""

df5.show(10,truncate=True)

"""Truncate means if there are large number of records in the database and we want to show the data if the data is false it will show .... if it is true it will show all of the data."""

df5.show(10,truncate=False)

df6.show(20)

df_no_duplicates=df6.dropDuplicates()

df_no_duplicates.show()

sorted_df = df_no_duplicates.orderBy("quiz")

# Show the sorted DataFrame
sorted_df.show()

df6.show()

#In PySpark, the distinct() function is used to remove duplicate rows from a DataFrame.
#It returns a new DataFrame with distinct rows.
df6.distinct().show(20)

df.show(5)

df.select("age").show(5)

#Filter operations
#Column rename

df = spark.read.option("delimiter",' ').csv("/content/500.txt",header=True,inferSchema=True)
df.show()
df.printSchema()

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Create a SparkSession
spark = SparkSession.builder \
    .appName("CreateDataFrameExample") \
    .getOrCreate()

# Define schema for the DataFrame
schema = StructType([
    StructField("CountryName", StringType(), True),
    StructField("CountryCode", StringType(), True),
    StructField("Year", IntegerType(), True),
    StructField("value", IntegerType(), True)
])

# Sample data
data = [("India", "IND", 2011,1),
        ("Japan", "JPN", 2012,2),
        ("china", "CHA", 2013,3)]

# Create a DataFrame
df = spark.createDataFrame(data, schema)

# Show the DataFrame
df.show()

df = spark.read.options(delimiter='|',infeschema=True,header=True).csv("/content/file.csv.txt")
df.show()
df.printSchema()

"""Filter is used to filter the things
#Condition and year is 2000 or 2005
"""

from pyspark.sql.functions import col
filtered_dataframe =df.filter(col('Year').isin(2000,2005) )
filtered_dataframe.show()

"""# Condition for year in 2000 and 2005"""

from pyspark.sql.functions import col
filtered_dataframe = df.filter((col('Year') == 2000) & (col('Year') == 2005))
filtered_dataframe.show()
# Empty result because there is no matching column

"""#Filter in Pyspark

"""

from pyspark.sql.functions import col
filtered_dataframe =df.filter(col('Country Name')=='India' )
filtered_dataframe.show()

"""# Do filter,groupBy and aggregation all in one function."""

new_data_frame = df.filter(col('Year') == 2000).groupby("Value  ").agg({'Value  ':'count'})
new_data_frame.show()

"""# Column Renamed  Function"""

# new_data={'country':'countryname'}
df =df.withColumnRenamed("Country Name","Country")
# df_renamed.show()
df.show()

"""change multiple column names"""



"""## Partitioning and shuffling
where we can use RDD and data frame

"""

