{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "LqiGy5PLZpDB",
        "outputId": "95201929-fc1a-4cbd-8f56-70e386ebb5c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7bc4e8f208b0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://8e0d1506fd1d:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Transformation Operations:**\n",
        "   a. Write a Spark application to read a text file and convert all words to uppercase using the `map()` transformation.\n",
        "\n",
        "   b. Given an RDD of integers, filter out numbers greater than 10 using the `filter()` transformation.\n",
        "\n",
        "   c. Concatenate two RDDs containing strings using the `union()` transformation.\n",
        "\n",
        "   d. Perform a `flatMap()` transformation on an RDD of sentences to split each sentence into words.\n",
        "   \n",
        "   e. Use the `distinct()` transformation to remove duplicate elements from an RDD of integers."
      ],
      "metadata": {
        "id": "XS9aEbPUdQcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.textFile(\"/content/rdd operations.txt\")\n",
        "rdd1=rdd.flatMap(lambda x:x.split('\\\\s+'))\n",
        "rdd2 =rdd1.map(lambda x: x.strip())\n",
        "import re\n",
        "\n",
        "pattern = r'[,;|!*:.()`]'\n",
        "rdd2 = rdd2.flatMap(lambda x:re.split(pattern,x))\n",
        "rdd2=rdd2.flatMap(lambda x:x.split(' '))\n",
        "rdd2 = rdd2.filter(lambda x:x!='')\n",
        "\n",
        "rddx = rdd2.map(lambda x:(x,1)).reduceByKey(lambda a,b:a+b).map(lambda x:(x[1],x[0])).sortByKey(False).map(lambda x:(x[1],x[0]))\n",
        "rddx.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9h3D2szZptk",
        "outputId": "0963db75-05b5-459f-a89c-fbc7ac4996d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 34),\n",
              " ('a', 20),\n",
              " ('RDD', 20),\n",
              " ('to', 19),\n",
              " ('using', 18),\n",
              " ('of', 16),\n",
              " ('transformation', 12),\n",
              " ('an', 11),\n",
              " ('and', 11),\n",
              " ('for', 8),\n",
              " ('Pair', 8),\n",
              " ('b', 6),\n",
              " ('c', 6),\n",
              " ('d', 6),\n",
              " ('Use', 6),\n",
              " ('in', 6),\n",
              " ('RDDs', 6),\n",
              " ('e', 6),\n",
              " ('elements', 6),\n",
              " ('action', 5),\n",
              " ('function', 5),\n",
              " ('optimize', 4),\n",
              " ('Operations', 4),\n",
              " ('on', 4),\n",
              " ('from', 4),\n",
              " ('key', 4),\n",
              " ('Spark', 3),\n",
              " ('integers', 3),\n",
              " ('two', 3),\n",
              " ('operations', 3),\n",
              " ('Implement', 3),\n",
              " ('each', 3),\n",
              " ('values', 3),\n",
              " ('with', 3),\n",
              " ('or', 3),\n",
              " ('create', 3),\n",
              " ('custom', 3),\n",
              " ('questions', 2),\n",
              " ('into', 2),\n",
              " ('sum', 2),\n",
              " ('reduce', 2),\n",
              " ('measure', 2),\n",
              " ('different', 2),\n",
              " ('exceptions', 2),\n",
              " ('fault', 2),\n",
              " ('tolerance', 2),\n",
              " ('overhead', 2),\n",
              " ('perform', 2),\n",
              " ('practice', 2),\n",
              " ('text', 2),\n",
              " ('file', 2),\n",
              " ('all', 2),\n",
              " ('words', 2),\n",
              " ('filter', 2),\n",
              " ('number', 2),\n",
              " ('5', 2),\n",
              " ('by', 2),\n",
              " ('time', 2),\n",
              " ('taken', 2),\n",
              " ('partitions', 2),\n",
              " ('data', 2),\n",
              " ('checkpoint', 2),\n",
              " ('Certainly', 1),\n",
              " ('are', 1),\n",
              " ('1', 1),\n",
              " ('read', 1),\n",
              " ('convert', 1),\n",
              " ('uppercase', 1),\n",
              " ('out', 1),\n",
              " ('numbers', 1),\n",
              " ('than', 1),\n",
              " ('10', 1),\n",
              " ('remove', 1),\n",
              " ('Action', 1),\n",
              " ('Calculate', 1),\n",
              " ('Find', 1),\n",
              " ('value', 1),\n",
              " ('floats', 1),\n",
              " ('max', 1),\n",
              " ('Count', 1),\n",
              " ('count', 1),\n",
              " ('array', 1),\n",
              " ('take', 1),\n",
              " ('Save', 1),\n",
              " ('contents', 1),\n",
              " ('Join', 1),\n",
              " ('based', 1),\n",
              " ('join', 1),\n",
              " ('Group', 1),\n",
              " ('Sort', 1),\n",
              " ('cogroup', 1),\n",
              " ('combine', 1),\n",
              " ('4', 1),\n",
              " ('Performance', 1),\n",
              " ('cache', 1),\n",
              " ('persist', 1),\n",
              " ('subsequent', 1),\n",
              " ('broadcast', 1),\n",
              " ('large', 1),\n",
              " ('read-only', 1),\n",
              " ('worker', 1),\n",
              " ('nodes', 1),\n",
              " ('coalesce', 1),\n",
              " ('parallelize', 1),\n",
              " ('collection', 1),\n",
              " ('executor/memory', 1),\n",
              " ('settings', 1),\n",
              " ('performance', 1),\n",
              " ('Error', 1),\n",
              " ('Handling', 1),\n",
              " ('Fault', 1),\n",
              " ('Tolerance', 1),\n",
              " ('gracefully', 1),\n",
              " ('when', 1),\n",
              " ('external', 1),\n",
              " ('like', 1),\n",
              " ('HDFS', 1),\n",
              " ('S3', 1),\n",
              " ('try-catch', 1),\n",
              " ('blocks', 1),\n",
              " ('handle', 1),\n",
              " ('specific', 1),\n",
              " ('Monitor', 1),\n",
              " ('job', 1),\n",
              " ('progress', 1),\n",
              " ('resource', 1),\n",
              " ('usage', 1),\n",
              " ('identify', 1),\n",
              " ('errors', 1),\n",
              " ('failed', 1),\n",
              " ('tasks', 1),\n",
              " (\"Spark's\", 1),\n",
              " ('mechanisms', 1),\n",
              " ('distribution', 1),\n",
              " ('aggregations', 1),\n",
              " ('Combine', 1),\n",
              " ('schemas', 1),\n",
              " ('zip', 1),\n",
              " ('paired', 1),\n",
              " ('serialization', 1),\n",
              " ('format', 1),\n",
              " ('storage', 1),\n",
              " ('network', 1),\n",
              " ('foreachPartition', 1),\n",
              " ('These', 1),\n",
              " ('concepts', 1),\n",
              " ('Apache', 1),\n",
              " ('providing', 1),\n",
              " ('opportunities', 1),\n",
              " ('hands-on', 1),\n",
              " ('learning', 1),\n",
              " ('development', 1),\n",
              " ('Here', 1),\n",
              " ('specified', 1),\n",
              " ('topics', 1),\n",
              " ('Transformation', 1),\n",
              " ('Write', 1),\n",
              " ('application', 1),\n",
              " ('map', 1),\n",
              " ('Given', 1),\n",
              " ('greater', 1),\n",
              " ('Concatenate', 1),\n",
              " ('containing', 1),\n",
              " ('strings', 1),\n",
              " ('union', 1),\n",
              " ('Perform', 1),\n",
              " ('flatMap', 1),\n",
              " ('sentences', 1),\n",
              " ('split', 1),\n",
              " ('sentence', 1),\n",
              " ('distinct', 1),\n",
              " ('duplicate', 1),\n",
              " ('2', 1),\n",
              " ('maximum', 1),\n",
              " ('Collect', 1),\n",
              " ('first', 1),\n",
              " ('local', 1),\n",
              " ('saveAsTextFile', 1),\n",
              " ('3', 1),\n",
              " ('Compute', 1),\n",
              " ('reduceByKey', 1),\n",
              " ('their', 1),\n",
              " ('keys', 1),\n",
              " ('groupByKey', 1),\n",
              " ('sortByKey', 1),\n",
              " ('same', 1),\n",
              " ('Optimization', 1),\n",
              " ('Cache', 1),\n",
              " ('memory', 1),\n",
              " ('actions', 1),\n",
              " ('efficiently', 1),\n",
              " ('distribute', 1),\n",
              " ('variables', 1),\n",
              " ('Adjust', 1),\n",
              " ('repartition', 1),\n",
              " ('parallelism', 1),\n",
              " ('in-memory', 1),\n",
              " ('Experiment', 1),\n",
              " ('cluster', 1),\n",
              " ('configurations', 1),\n",
              " ('Handle', 1),\n",
              " ('reading', 1),\n",
              " ('sources', 1),\n",
              " ('lineage', 1),\n",
              " ('error', 1),\n",
              " ('handling', 1),\n",
              " ('logic', 1),\n",
              " ('UI', 1),\n",
              " ('troubleshoot', 1),\n",
              " ('Retry', 1),\n",
              " ('automatically', 1),\n",
              " ('6', 1),\n",
              " ('Advanced', 1),\n",
              " ('partitioner', 1),\n",
              " ('shuffle', 1),\n",
              " ('aggregateByKey', 1),\n",
              " ('complex', 1),\n",
              " ('Utilize', 1),\n",
              " ('efficient', 1),\n",
              " ('bulk', 1),\n",
              " ('cover', 1),\n",
              " ('wide', 1),\n",
              " ('range', 1),\n",
              " ('skill', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "rdd = spark.sparkContext.parallelize(np.random.randint(0,989,size=60))\n",
        "rdd.collect()\n",
        "rdd=rdd.filter(lambda x:x>300)\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUrGXWJ2dKB8",
        "outputId": "583d7b40-6357-4082-fc39-9e4dd6c98edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[954,\n",
              " 687,\n",
              " 981,\n",
              " 826,\n",
              " 850,\n",
              " 907,\n",
              " 587,\n",
              " 785,\n",
              " 517,\n",
              " 430,\n",
              " 486,\n",
              " 515,\n",
              " 438,\n",
              " 468,\n",
              " 597,\n",
              " 863,\n",
              " 673,\n",
              " 920,\n",
              " 558,\n",
              " 819,\n",
              " 914,\n",
              " 335,\n",
              " 582,\n",
              " 577,\n",
              " 772,\n",
              " 927,\n",
              " 439,\n",
              " 944,\n",
              " 864,\n",
              " 484,\n",
              " 960,\n",
              " 440,\n",
              " 964,\n",
              " 818,\n",
              " 607,\n",
              " 302,\n",
              " 668,\n",
              " 915,\n",
              " 724,\n",
              " 752,\n",
              " 453,\n",
              " 753,\n",
              " 325]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = spark.sparkContext.parallelize([\"yatendra\",\"taran\",\"kanisk\",\"neel\"])\n",
        "rdd2 = spark.sparkContext.parallelize([\"purunjay\",\"janit\",\"pranshu\",\"yatendra\"])\n",
        "\n",
        "rdd1.union(rdd2).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FxKk3SUw7pE",
        "outputId": "52fa14a1-3e61-43db-e90a-192ee9244e42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yatendra',\n",
              " 'taran',\n",
              " 'kanisk',\n",
              " 'neel',\n",
              " 'purunjay',\n",
              " 'janit',\n",
              " 'pranshu',\n",
              " 'yatendra']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize(np.random.randint(0,10,size=16))\n",
        "rdd.distinct().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mR8BZJZzAlv",
        "outputId": "e57932c1-1c94-4874-e63f-32ee5500522e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 8, 6, 0, 9, 5, 7, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = spark.sparkContext.parallelize([(1,\"Yatendra\"),(2,\"taran\"),(3,\"neels\"),(4,\"hash\")]);\n",
        "rdd2 = spark.sparkContext.parallelize([(1,22),(2,21),(3,24),(4,45)])\n",
        "\n",
        "rdd1.join(rdd2).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP8MSalY04aF",
        "outputId": "84a4efa8-ad95-4c97-aa06-4b309f225972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(4, ('hash', 45)),\n",
              " (1, ('Yatendra', 22)),\n",
              " (2, ('taran', 21)),\n",
              " (3, ('neels', 24))]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Action Operations:**\n",
        "\n",
        "   a. Calculate the sum of all elements in an RDD of integers using the `reduce()` action.\n",
        "\n",
        "   b. Find the maximum value in an RDD of floats using the `max()` action.\n",
        "\n",
        "   c. Count the number of elements in an RDD using the `count()` action.\n",
        "\n",
        "   d. Collect the first 5 elements of an RDD into a local array using the `take()` action.\n",
        "\n",
        "   e. Save the contents of an RDD to a text file using the `saveAsTextFile()` action.\n",
        "\n",
        "\n",
        "```\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Qz6RSAX-8ODY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize(np.random.randint(0,20,size=10))\n",
        "print(\"rdd = \"+str(rdd.collect()))\n",
        "print(\"sum = \"+str(rdd.sum()))\n",
        "print(\"max = \",rdd.max())\n",
        "print(\"count = \",rdd.count())\n",
        "arr= rdd.take(5)\n",
        "print(\"array\",arr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPIruzSK2hKN",
        "outputId": "9dae9c1b-2a16-4661-bd8a-3f05b1f0667f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rdd = [19, 11, 18, 10, 9, 10, 17, 17, 16, 17]\n",
            "sum = 144\n",
            "max =  19\n",
            "count =  10\n",
            "array [19, 11, 18, 10, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.saveAsTextFile(\"/content/NEWoutput.txt\")"
      ],
      "metadata": {
        "id": "H4JiNxOT8oTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Pair RDD Operations:**\n",
        "\n",
        "   a. Compute the sum of values for each key in a Pair RDD using the `reduceByKey()` transformation.\n",
        "\n",
        "   b. Join two Pair RDDs based on their keys using the `join()` transformation.\n",
        "\n",
        "   c. Group the values of a Pair RDD by key using the `groupByKey()` transformation.\n",
        "\n",
        "   d. Sort the elements of a Pair RDD by key using the `sortByKey()` transformation.\n",
        "   \n",
        "   e. Use the `cogroup()` transformation to combine values from two Pair RDDs with the same key."
      ],
      "metadata": {
        "id": "1wMYifbCCFWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = spark.sparkContext.parallelize([(\"yat\",10),(\"yat\",2),(\"tar\",9),('kan',8),('tar',1),('neel',6),('kan',7)])\n",
        "rdd1 = rdd1.reduceByKey(lambda a,b:a+b)\n",
        "rdd1.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9I3JzLZBqti",
        "outputId": "3227c6cb-1ddd-4eb1-88e7-013497ae5989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('yat', 12), ('tar', 10), ('neel', 6), ('kan', 15)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2 = spark.sparkContext.parallelize([(\"yat\",10),('tar',7),('kan',4)])\n",
        "\n",
        "rddres =rdd1.join(rdd2)\n",
        "rddres.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p3TqFyIWlKL",
        "outputId": "afd2555c-8747-48a8-b0e6-fcdc30f8fb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('yat', (12, 10)), ('kan', (15, 4)), ('tar', (10, 7))]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with mapValues function(we can pass any iterable in mapvalues)\n",
        "rdd1 = spark.sparkContext.parallelize([(\"yat\",10),(\"yat\",2),(\"tar\",9),('kan',8),('tar',1),('neel',6),('kan',7)])\n",
        "rdd1 = rdd1.groupByKey().mapValues(list)\n",
        "print(rdd1.collect())\n",
        "\n",
        "#without using mapvalue function\n",
        "rdd1 = spark.sparkContext.parallelize([(\"yat\",10),(\"yat\",2),(\"tar\",9),('kan',8),('tar',1),('neel',6),('kan',7)])\n",
        "rdd1 = rdd1.groupByKey()\n",
        "result = rdd1.collect()\n",
        "for name, marks in result:\n",
        "    print(f\"{name}: {list(marks)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_nfhYTOXEFi",
        "outputId": "74abbd43-96e1-463e-f3a3-e706b6251b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('yat', [10, 2]), ('tar', [9, 1]), ('neel', [6]), ('kan', [8, 7])]\n",
            "yat: [10, 2]\n",
            "tar: [9, 1]\n",
            "neel: [6]\n",
            "kan: [8, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sortbykey\n",
        "rdd1 = spark.sparkContext.parallelize([(\"yat\",10),(\"yat\",2),(\"tar\",9),('kan',8),('tar',1),('neel',6),('kan',7)])\n",
        "rdd1 = rdd1.sortByKey()\n",
        "rdd1.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K_StGPuXdPW",
        "outputId": "a33190eb-61d7-4881-f5e2-a7959db3b3e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('kan', 8),\n",
              " ('kan', 7),\n",
              " ('neel', 6),\n",
              " ('tar', 9),\n",
              " ('tar', 1),\n",
              " ('yat', 10),\n",
              " ('yat', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cogroup\n",
        "rdd1 = spark.sparkContext.parallelize([(\"yat\",10),(\"yat\",2),(\"tar\",9),('kan',8),('tar',1),('neel',6),('kan',7)])\n",
        "rdd2 = spark.sparkContext.parallelize([(\"yat\",17),('tar',7),('kan',4)])\n",
        "\n",
        "rddx = rdd1.cogroup(rdd2)\n",
        "rddx = rddx.collect()\n",
        "\n",
        "for key, values in rddx:\n",
        "    print(f\"Key: {key}, Values: {list(map(list, values))}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3TavjTfvaQf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f0aff05-f210-4146-9238-1077d204b8c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key: yat, Values: [[10, 2], [17]]\n",
            "Key: neel, Values: [[6], []]\n",
            "Key: kan, Values: [[8, 7], [4]]\n",
            "Key: tar, Values: [[9, 1], [7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Performance Optimization:**\n",
        "\n",
        "   a. Cache an RDD in memory using the `cache()` or `persist()` function and measure the time taken for subsequent actions.\n",
        "\n",
        "   b. Use the `broadcast()` function to efficiently distribute large read-only variables to worker nodes.\n",
        "\n",
        "   c. Adjust the number of partitions using the `repartition()` or `coalesce()` function to optimize parallelism.\n",
        "\n",
        "   d. Use `parallelize()` to create an RDD from an in-memory collection and measure the time taken for operations.\n",
        "   \n",
        "   e. Experiment with different cluster configurations and executor/memory settings to optimize performance."
      ],
      "metadata": {
        "id": "BWyd9PgUhVeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### cache and persist do same thing it's just cache stores data in memory\n",
        "### and persist can store it in 5 diffrent types of storage\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize([(\"yat\",10),(\"yat\",2),(\"tar\",9),('kan',8),('tar',1),('neel',6),('kan',7)])\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "persistMemory = rdd1.persist()\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Time taken while persist = {end_time-start_time}\")\n",
        "\n",
        "start_time = time.time()\n",
        "cacheMemory = rdd1.cache()\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Time taken while caching = {end_time-start_time}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpE-j0mjhb_V",
        "outputId": "ebe2ff0d-4749-457b-b21b-1871778d7752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken while persist = 0.0072290897369384766\n",
            "Time taken while caching = 0.00466156005859375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Broadcast variable\n",
        "## Broadcast variables are read-only shared variables that are cached and available\n",
        "## on all nodes in a cluster in-order to access or use by the tasks. Instead of\n",
        "## sending this data along with every task, PySpark distributes broadcast variables\n",
        "## to the workers using efficient broadcast algorithms to reduce communication costs.\n",
        "\n",
        "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
        "broadcastStates = spark.sparkContext.broadcast(states)\n",
        "\n",
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "  ]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "def state_convert(code):\n",
        "    return broadcastStates.value[code]\n",
        "\n",
        "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhnznGWpjuLN",
        "outputId": "dab08473-6102-4112-88bc-fac056ad9578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# repartition and coalesce\n",
        "\n",
        "repartitionedRDD = rdd.repartition(4)\n",
        "print(repartitionedRDD.glom().collect())\n",
        "\n",
        "## Coalesce the RDD into 2 partitions\n",
        "coalescedRDD = rdd.coalesce(3)\n",
        "\n",
        "## Action to view the partitions\n",
        "coalescedRDD.glom().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYpHaj1R1dDK",
        "outputId": "ad3909a3-2d43-4d00-c272-6965304802e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[], [('Robert', 'Williams', 'USA', 'CA'), ('Maria', 'Jones', 'USA', 'FL')], [], [('James', 'Smith', 'USA', 'CA'), ('Michael', 'Rose', 'USA', 'NY')]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('James', 'Smith', 'USA', 'CA'), ('Michael', 'Rose', 'USA', 'NY')],\n",
              " [('Robert', 'Williams', 'USA', 'CA'), ('Maria', 'Jones', 'USA', 'FL')]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "  ]\n",
        "\n",
        "start_time = time.time()\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "end_time = time.time()\n",
        "print(end_time-start_time)\n",
        "\n",
        "cacheRDD = rdd.cache()\n",
        "\n",
        "start_time = time.time()\n",
        "newRDD = spark.sparkContext.parallelize(cacheRDD.collect())\n",
        "end_time = time.time()\n",
        "\n",
        "print(end_time-start_time)\n",
        "newRDD.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWqyxC8JW8Mg",
        "outputId": "1f4d673f-c1e2-4b60-aecd-796285763905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.005979299545288086\n",
            "0.040570974349975586\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('James', 'Smith', 'USA', 'CA'),\n",
              " ('Michael', 'Rose', 'USA', 'NY'),\n",
              " ('Robert', 'Williams', 'USA', 'CA'),\n",
              " ('Maria', 'Jones', 'USA', 'FL')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7H1UFJfebtoU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}