{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "mOhUnbp1ZWhN",
        "outputId": "d3e6046b-c14a-4c18-9ed2-347e0016be87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e4282c1a6b0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f464c1095326:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFuQhxgwpXgC",
        "outputId": "dd0cd280-bf77-4663-e249-5d8562614a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+----+-----+\n",
            "|CountryCode|  Name|Year|Value|\n",
            "+-----------+------+----+-----+\n",
            "|          1|   USA|  30|    1|\n",
            "|          2| India|  25|    1|\n",
            "|          3|Russia|  35|    1|\n",
            "+-----------+------+----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "data = [\n",
        "    Row(CountryCode=1, Name='USA', Year=30 , Value=1),\n",
        "    Row(CountryCode=2, Name='India', Year=25, Value=1),\n",
        "    Row(CountryCode=3, Name='Russia', Year=35, Value=1)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "jhfubKraqa6d",
        "outputId": "7ebdf54e-2242-41b7-d801-280b4c8c5354"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Path does not exist: file:/content/file.txt",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5078a9d11b52>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/file.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/file.txt"
          ]
        }
      ],
      "source": [
        "df2 = spark.read.options(delimiter='|', inferschema=True, header=True).csv(\"/content/file.txt\")\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHzkgmAWrfj7"
      },
      "outputs": [],
      "source": [
        "df2.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsilZmVcrvhP"
      },
      "outputs": [],
      "source": [
        "df2.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twN3unYpr-pY"
      },
      "outputs": [],
      "source": [
        "#Find the number of rows in dataframe\n",
        "df2.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5djCJ5nctZ8V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfmb5nmTsD_2"
      },
      "outputs": [],
      "source": [
        "df3 = spark.read.options(delimiter='|', inferschema=True, header=True).csv(\"/content/file.txt\")\n",
        "df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCNF06m8s6FF"
      },
      "outputs": [],
      "source": [
        "unique_df=df3.dropDuplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34v77XMduPYO"
      },
      "outputs": [],
      "source": [
        "unique_df.show()\n",
        "df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swiO2c-RwnNY"
      },
      "outputs": [],
      "source": [
        "#Try to avoid this way instaed go for below option\n",
        "df2.show()\n",
        "filtered_df = df2.filter(df2[\"Country Name\"] ==\"India\")\n",
        "filtered_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxxobJqhw5sD"
      },
      "outputs": [],
      "source": [
        "#Filter column\n",
        "from pyspark.sql.functions import col\n",
        "filtered_df = df2.filter(col(\"Country Name\")==\"India\")\n",
        "filtered_df.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ques1. Filter the colums where country is India and year is 2000 and 2005 using & and | operators"
      ],
      "metadata": {
        "id": "mDnXjNT0bRC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_df.show()\n",
        "filtered_df=unique_df.filter((col(\"Country Name\")==\"India\") |((col(\"Year\")==2000) | (col(\"Year\")==2005)))\n",
        "filtered_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "b69GySFDblGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.orderBy(\"Value\",ascending=False).show()\n"
      ],
      "metadata": {
        "id": "T5s0tBbTZ2xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmgXaQnryj7K"
      },
      "outputs": [],
      "source": [
        "df2.select(\"Value\").orderBy(\"Value\",ascending=False).show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, sum, avg, min, max, mean\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"John\", 1000),\n",
        "        (\"Alice\", 2000),\n",
        "        (\"Bob\", 1500),\n",
        "        (\"Alice\", 3000)]\n",
        "\n",
        "columns = [\"Name\", \"Salary\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Perform aggregation\n",
        "df.agg(\n",
        "    count(col(\"Name\")).alias(\"TotalCount\"),\n",
        "    sum(col(\"Salary\")).alias(\"TotalSalary\"),\n",
        "    avg(col(\"Salary\")).alias(\"AverageSalary\"),\n",
        "    min(col(\"Salary\")).alias(\"MinSalary\"),\n",
        "    max(col(\"Salary\")).alias(\"MaxSalary\"),\n",
        "    mean(col(\"Salary\")).alias(\"MeanSalary\")\n",
        ").show()\n"
      ],
      "metadata": {
        "id": "Szliw0RgzMwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##filter,groupby, aggregation all in one function"
      ],
      "metadata": {
        "id": "j8_80d5waJCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum, count\n",
        "\n",
        "data = [(\"John\", \"Sales\", 1000),\n",
        "        (\"Alice\", \"HR\", 2000),\n",
        "        (\"Bob\", \"Sales\", 1500),\n",
        "        (\"Alice\", \"Sales\", 3000)]\n",
        "\n",
        "columns = [\"Name\", \"Department\", \"Salary\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "\n",
        "grouped_df = df.filter(df[\"Department\"] == \"Sales\").groupBy(\"Name\").agg(\n",
        "    sum(col(\"Salary\")).alias(\"TotalSalary\"),\n",
        "    count(col(\"Name\")).alias(\"EmployeeCount\")\n",
        ")\n",
        "grouped_df.show()"
      ],
      "metadata": {
        "id": "lRzu6rNrVeLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show()\n",
        "final_df=(df2.filter((col(\"Year\")==\"2000\") | (col(\"Year\")==\"2003\"))\n",
        "    .groupBy(\"Country Name\")\n",
        "    .agg(count(col(\"Country Name\")).alias(\"CountryCount\"),\n",
        "      sum(col(\"Value\")).alias(\"ValueCount\")))\n",
        "final_df.show()"
      ],
      "metadata": {
        "id": "WqkwjJjFaUya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.show()"
      ],
      "metadata": {
        "id": "GEiQlhzjkxtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4B0qnsezlCL"
      },
      "outputs": [],
      "source": [
        "df_renamed = df3.withColumnRenamed(\"Country Name\", \"Country\")\n",
        "df_renamed.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7nj212Vz6xt"
      },
      "outputs": [],
      "source": [
        "# Partition and shuffling\n",
        "#Wwhere we can use rdd amd where dataset\n",
        "job\n",
        "shared folder\n",
        "config file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Folder structure\n",
        "=>Jobs-Pyspark Code(Read data from diff files)(Dump data to warehouse)\n",
        "=>Shared => scripts\n",
        "=>Configs\n",
        "\n",
        "Data Dumps=>db2,postgres to dataframe"
      ],
      "metadata": {
        "id": "yvgh-wOVWt6q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tS3z4ZWwYA1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Properties\n",
        "1.num executors\n",
        "2. Executor memory 10gb\n",
        "3. executor cores\n",
        "4. Driver memory"
      ],
      "metadata": {
        "id": "-yXFL4-JYCBo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iEiK8VkGWocF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 19 March\n"
      ],
      "metadata": {
        "id": "sC0mu1Moq_ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "SparkSession.sparkContext.setLogLevel('Error')"
      ],
      "metadata": {
        "id": "UpeUqmqqrEPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def func():\n",
        "  df=spark.read.csv(\"/content/Foods.csv\", header=True)\n",
        "\n",
        "  logger.info(\"Successfully Created\")\n",
        "\n",
        "func()"
      ],
      "metadata": {
        "id": "Ez7Kxwr2ra1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "def func():\n",
        "    try:\n",
        "        df = spark.read.csv(\"/content/Foods.csv\", header=True)\n",
        "        df.show()\n",
        "        logging.info(\"Successfully created\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred: {str(e)}\")\n",
        "    finally:\n",
        "      spark.stop()\n",
        "\n",
        "# Call the function\n",
        "func()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA0sKf6ltXvY",
        "outputId": "ce942b30-fe77-4970-ca68-412d0662b327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Successfully created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----+\n",
            "|Food ID| Food Item|Price|\n",
            "+-------+----------+-----+\n",
            "|      1|     Sushi| 3.99|\n",
            "|      2|   Burrito| 9.99|\n",
            "|      3|      Taco| 2.99|\n",
            "|      4|Quesadilla| 4.25|\n",
            "|      5|     Pizza| 2.49|\n",
            "|      6|     Pasta|13.99|\n",
            "|      7|     Steak|24.99|\n",
            "|      8|     Salad|11.25|\n",
            "|      9|     Donut| 0.99|\n",
            "|     10|     Drink| 1.75|\n",
            "+-------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3fVUmZ_NuTZ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}